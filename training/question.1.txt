Question 1:
The topic for my classifier was dogs, which allowed for lots of simple statementss that could be easily interpreted by python. I chose my positive features before I began writing my positive and negative training instances as a way to help determine helpful training data, and made sure to include both true positive and false positive statements with these words in them. My first set of words was "dog, walk, pet, fur, bark". When I first ran the training instances through the classifier, it calculated a 0.55 accuracy, so I added a few more positive features to make my list include plural forms of some words, and introduced the features "puppy collar fetch". This raised my accuracy to 0.65, so I added a few more true positive training instances and a few more true negative instances, which allowed me to raise it over 0.7. When determining which words to add, I strategically looked for words that occured often in my positive training data and not very often in my negative training data, and that related to my topic. While some words occured often in the positive data, such as "Charlie" (the name of my hypothetical dog that allowed me to write sentences without saying "dog" each time) was not relavent to the training data as the name Charlie is not specific to dogs. This is the same thought process as with filler words such as "I", and why such words were not used in my positive features. My training data attempted to reflect phrases commonly associated with all dogs, and not cenetered around any specific dog. 